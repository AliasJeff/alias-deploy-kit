# run.py
import os
import json
import time
import torch
import psutil
import logging
import numpy as np
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from config import Config


class BenchmarkRunner:

    def __init__(self):
        self.cfg = Config()
        self.results = []

        os.makedirs(self.cfg.RESULT_DIR, exist_ok=True)
        os.makedirs(self.cfg.LOG_DIR, exist_ok=True)

        self.logger = self.setup_logger()
        self.logger.info(f"ğŸš€ åˆå§‹åŒ–åŸºå‡†æµ‹è¯•ï¼Œè®¾å¤‡: {self.cfg.DEVICE}")

        # æ£€æŸ¥ 4bit å…¼å®¹æ€§è­¦å‘Š
        if self.cfg.LOAD_IN_4BIT and self.cfg.DEVICE != "cuda":
            self.logger.warning(
                "âš ï¸ æ£€æµ‹åˆ°å¼€å¯äº† LOAD_IN_4BIT ä½†è®¾å¤‡ä¸æ˜¯ CUDAã€‚bitsandbytes å¯èƒ½æ— æ³•å·¥ä½œã€‚")

    def setup_logger(self):
        today_str = datetime.now().strftime("%Y-%m-%d")
        log_file = os.path.join(self.cfg.LOG_DIR, f"{today_str}.log")

        logger = logging.getLogger("LLM_Benchmark")
        logger.setLevel(logging.INFO)

        if logger.hasHandlers():
            logger.handlers.clear()

        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')

        file_handler = logging.FileHandler(log_file,
                                           mode='a',
                                           encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

        return logger

    def get_directory_size(self, start_path):
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(start_path):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                if not os.path.islink(fp):
                    total_size += os.path.getsize(fp)
        return total_size / (1024**3)

    def get_memory_usage(self):
        mem_info = {}
        process = psutil.Process(os.getpid())
        mem_info['ram_usage_mb'] = process.memory_info().rss / 1024 / 1024

        if self.cfg.DEVICE == "cuda":
            mem_info['gpu_vram_mb'] = torch.cuda.memory_allocated(
            ) / 1024 / 1024
            mem_info['gpu_vram_max_mb'] = torch.cuda.max_memory_allocated(
            ) / 1024 / 1024
        return mem_info

    def load_model(self):
        self.logger.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.cfg.MODEL_PATH} ...")

        # æ„é€ é‡åŒ–é…ç½®
        bnb_config = None
        if self.cfg.LOAD_IN_4BIT:
            self.logger.info("ğŸ”§ å·²å¯ç”¨ 4-bit é‡åŒ–åŠ è½½ (NF4)")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",  # æ¨èä½¿ç”¨ nf4 æ ¼å¼
                bnb_4bit_use_double_quant=True,  # å¼€å¯åŒé‡é‡åŒ–ä»¥èŠ‚çœæ›´å¤šæ˜¾å­˜
                bnb_4bit_compute_dtype=self.cfg.
                TORCH_DTYPE  # è®¡ç®—æ—¶ä½¿ç”¨çš„ç²¾åº¦ (fp16/bf16)
            )

        start_time = time.time()

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.cfg.MODEL_PATH, trust_remote_code=True)

            # æ³¨æ„: ä½¿ç”¨ load_in_4bit æ—¶ï¼Œå»ºè®® device_map="auto" æˆ–è€…ç”± accelerate è‡ªåŠ¨å¤„ç†
            self.model = AutoModelForCausalLM.from_pretrained(
                self.cfg.MODEL_PATH,
                device_map="auto"
                if self.cfg.LOAD_IN_4BIT else self.cfg.DEVICE,
                quantization_config=bnb_config,
                torch_dtype=self.cfg.TORCH_DTYPE,
                trust_remote_code=True)
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.logger.error(f"è¯·æ£€æŸ¥è·¯å¾„æˆ– bitsandbytes æ˜¯å¦å®‰è£…æ­£ç¡®")
            exit(1)

        load_time = time.time() - start_time
        self.logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")

        self.model_info = {
            "model_name": os.path.basename(self.cfg.MODEL_PATH),
            "model_size_gb": self.get_directory_size(self.cfg.MODEL_PATH),
            "quantization": "4bit" if self.cfg.LOAD_IN_4BIT else "None",
            "param_count": sum(p.numel() for p in self.model.parameters())
        }

    def load_data(self):
        try:
            with open(self.cfg.DATA_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.error(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®: {self.cfg.DATA_PATH}")
            exit(1)

    def run(self):
        self.load_model()
        data = self.load_data()

        # é¢„çƒ­
        if self.cfg.WARMUP_ROUNDS > 0:
            self.logger.info(f"ğŸ”¥ å¼€å§‹é¢„çƒ­ ({self.cfg.WARMUP_ROUNDS} è½®)...")
            try:
                # æ„é€ ç®€å•çš„è¾“å…¥
                dummy_input = self.tokenizer("Hello", return_tensors="pt").to(
                    self.model.device)
                for _ in range(self.cfg.WARMUP_ROUNDS):
                    self.model.generate(**dummy_input, max_new_tokens=10)
            except Exception as e:
                self.logger.warning(f"âš ï¸ é¢„çƒ­è¿‡ç¨‹ä¸­å‡ºç°å°é—®é¢˜ (å¯å¿½ç•¥): {e}")

        self.logger.info(f"âš¡ å¼€å§‹æ¨ç†ï¼Œå…± {len(data)} æ¡æµ‹è¯•æ•°æ®...")

        total_start_time = time.time()
        total_output_tokens = 0
        latencies = []

        for idx, item in enumerate(data):
            prompt = item['prompt']

            try:
                # 1. ç¼–ç 
                formatted_prompt = self.tokenizer.apply_chat_template(
                    [{
                        "role": "user",
                        "content": prompt
                    }],
                    tokenize=False,
                    add_generation_prompt=True,
                )

                # æ³¨æ„ï¼šç¡®ä¿è¾“å…¥ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                inputs = self.tokenizer(
                    [formatted_prompt],
                    return_tensors="pt",
                ).to(self.model.device)  # ä½¿ç”¨ model.device æ›´å®‰å…¨

                input_token_len = inputs.input_ids.shape[1]

                # 2. æ¨ç†
                if self.cfg.DEVICE == "cuda":
                    torch.cuda.reset_peak_memory_stats()

                t0 = time.perf_counter()
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=self.cfg.MAX_NEW_TOKENS,
                        temperature=self.cfg.TEMPERATURE,
                        top_p=self.cfg.TOP_P,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id,
                    )
                t1 = time.perf_counter()
                latency = t1 - t0
                latencies.append(latency)

                # 3. è§£ç 
                output_text = self.tokenizer.decode(
                    outputs[0][input_token_len:], skip_special_tokens=True)
                output_token_len = len(outputs[0]) - input_token_len

                total_output_tokens += output_token_len

                # é€Ÿåº¦è®¡ç®—
                tps = output_token_len / latency

                result_entry = {
                    "id": item['id'],
                    "prompt": prompt,
                    "output": output_text,
                    "metrics": {
                        "input_tokens": input_token_len,
                        "output_tokens": output_token_len,
                        "latency": round(latency, 4),
                        "tps": round(tps, 2),
                        "memory_stats": self.get_memory_usage()  # å®æ—¶è®°å½•å†…å­˜
                    }
                }
                self.results.append(result_entry)

                self.logger.info(
                    f"[{idx+1}/{len(data)}] ç”¨æ—¶: {latency:.2f}s | TPS: {tps:.2f} | Prompt: {prompt[:10]}..."
                )

            except Exception as e:
                self.logger.error(f"âŒ å¤„ç† ID {item['id']} æ—¶å‡ºé”™: {e}")

        total_duration = time.time() - total_start_time
        self.save_report(total_duration, total_output_tokens, latencies)

    def save_report(self, total_duration, total_output_tokens, latencies):
        if not latencies:
            self.logger.error("âŒ æ²¡æœ‰æˆåŠŸçš„æ¨ç†è®°å½•ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return

        avg_latency = np.mean(latencies)
        rps = len(self.results) / total_duration
        global_tps = total_output_tokens / total_duration

        report = {
            "meta": {
                "timestamp": datetime.now().isoformat(),
                "model": self.model_info,
                "config": {
                    k: v
                    for k, v in vars(self.cfg).items()
                    if not k.startswith("__")
                }
            },
            "summary": {
                "total_requests": len(self.results),
                "total_duration": round(total_duration, 2),
                "avg_latency": round(avg_latency, 4),
                "rps": round(rps, 2),
                "global_tps": round(global_tps, 2),
                "final_memory": self.results[-1]['metrics'].get('memory_stats')
            },
            "details": self.results
        }

        output_file = os.path.join(self.cfg.RESULT_DIR,
                                   f"benchmark_{int(time.time())}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        self.logger.info("=" * 30)
        self.logger.info("ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
        self.logger.info("=" * 30)
        self.logger.info(
            f"æ¨¡å‹é‡åŒ–: {self.model_info.get('quantization', 'None')}")
        self.logger.info(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.4f} s/req")
        self.logger.info(f"æ¨ç†åå: {global_tps:.2f} tokens/s")
        self.logger.info(f"è¯¦ç»†ç»“æœ: {output_file}")
        self.logger.info(
            f"æ—¥å¿—æ–‡ä»¶: {os.path.join(self.cfg.LOG_DIR, datetime.now().strftime('%Y-%m-%d') + '.log')}"
        )


if __name__ == "__main__":
    runner = BenchmarkRunner()
    runner.run()
