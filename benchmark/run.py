# run.py
import os
import json
import time
import torch
import psutil
import logging
import numpy as np
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from config import Config


class BenchmarkRunner:

    def __init__(self):
        self.cfg = Config()
        self.results = []

        os.makedirs(self.cfg.RESULT_DIR, exist_ok=True)
        os.makedirs(self.cfg.LOG_DIR, exist_ok=True)

        self.logger = self.setup_logger()
        self.logger.info(f"ğŸš€ åˆå§‹åŒ–åŸºå‡†æµ‹è¯•ï¼Œè®¾å¤‡: {self.cfg.DEVICE}")

        # æ£€æŸ¥ 4bit å…¼å®¹æ€§è­¦å‘Š
        if self.cfg.LOAD_IN_4BIT and self.cfg.DEVICE != "cuda":
            self.logger.warning(
                "âš ï¸ æ£€æµ‹åˆ°å¼€å¯äº† LOAD_IN_4BIT ä½†è®¾å¤‡ä¸æ˜¯ CUDAã€‚bitsandbytes å¯èƒ½æ— æ³•å·¥ä½œã€‚")

    def setup_logger(self):
        today_str = datetime.now().strftime("%Y-%m-%d")
        log_file = os.path.join(self.cfg.LOG_DIR, f"{today_str}.log")

        logger = logging.getLogger("LLM_Benchmark")
        logger.setLevel(logging.INFO)

        if logger.hasHandlers():
            logger.handlers.clear()

        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')

        file_handler = logging.FileHandler(log_file,
                                           mode='a',
                                           encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

        return logger

    def get_directory_size(self, start_path):
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(start_path):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                if not os.path.islink(fp):
                    total_size += os.path.getsize(fp)
        return total_size / (1024**3)

    def get_memory_usage(self):
        mem_info = {}
        process = psutil.Process(os.getpid())
        mem_info['ram_usage_mb'] = process.memory_info().rss / 1024 / 1024

        if self.cfg.DEVICE == "cuda":
            mem_info['gpu_vram_mb'] = torch.cuda.memory_allocated(
            ) / 1024 / 1024
            mem_info['gpu_vram_max_mb'] = torch.cuda.max_memory_allocated(
            ) / 1024 / 1024
        return mem_info

    def load_model(self):
        self.logger.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.cfg.MODEL_PATH} ...")

        # æ„é€ é‡åŒ–é…ç½®
        bnb_config = None
        if self.cfg.LOAD_IN_4BIT:
            self.logger.info("ğŸ”§ å·²å¯ç”¨ 4-bit é‡åŒ–åŠ è½½ (NF4)")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",  # æ¨èä½¿ç”¨ nf4 æ ¼å¼
                bnb_4bit_use_double_quant=True,  # å¼€å¯åŒé‡é‡åŒ–ä»¥èŠ‚çœæ›´å¤šæ˜¾å­˜
                bnb_4bit_compute_dtype=self.cfg.
                TORCH_DTYPE  # è®¡ç®—æ—¶ä½¿ç”¨çš„ç²¾åº¦ (fp16/bf16)
            )

        start_time = time.time()

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.cfg.MODEL_PATH, trust_remote_code=True)

            self.tokenizer.padding_side = 'left'
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.logger.info("ğŸ”§ Tokenizer ç¼ºå°‘ pad_tokenï¼Œå·²è‡ªåŠ¨è®¾ç½®ä¸º eos_token")

            is_awq = "awq" in self.cfg.MODEL_PATH.lower(
            ) or "marlin" in self.cfg.MODEL_PATH.lower()

            if is_awq:
                self.logger.info(
                    "ğŸ”§ æ£€æµ‹åˆ° AWQ/Marlin æ¨¡å‹ï¼Œä½¿ç”¨ AutoAWQForCausalLM åŠ è½½...")
                from awq import AutoAWQForCausalLM

                self.model = AutoAWQForCausalLM.from_pretrained(
                    self.cfg.MODEL_PATH,
                    low_cpu_mem_usage=True,
                    device_map="cuda",  # å¼ºåˆ¶ä½¿ç”¨ GPU
                    torch_dtype=self.cfg.TORCH_DTYPE,
                    trust_remote_code=True)
                self.device = self.model.model.device
            else:
                if self.cfg.LOAD_IN_4BIT:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.cfg.MODEL_PATH,
                        device_map="auto",
                        quantization_config=bnb_config,
                        torch_dtype=self.cfg.TORCH_DTYPE,
                        trust_remote_code=True)
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.cfg.MODEL_PATH,
                        device_map=self.cfg.DEVICE,
                        torch_dtype=self.cfg.TORCH_DTYPE,
                        trust_remote_code=True)
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            exit(1)

        load_time = time.time() - start_time
        self.logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")

        self.model_info = {
            "model_name": os.path.basename(self.cfg.MODEL_PATH),
            "model_size_gb": self.get_directory_size(self.cfg.MODEL_PATH),
            "quantization": "4bit" if self.cfg.LOAD_IN_4BIT else "None",
            "param_count": sum(p.numel() for p in self.model.parameters())
        }

    def load_data(self):
        try:
            with open(self.cfg.DATA_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.error(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®: {self.cfg.DATA_PATH}")
            exit(1)

    def run(self):
        self.load_model()
        data = self.load_data()

        # é¢„çƒ­
        if self.cfg.WARMUP_ROUNDS > 0:
            self.logger.info(f"ğŸ”¥ å¼€å§‹é¢„çƒ­ ({self.cfg.WARMUP_ROUNDS} è½®)...")
            try:
                # æ„é€ ç®€å•çš„è¾“å…¥
                dummy_input = self.tokenizer("Hello", return_tensors="pt").to(
                    self.device)
                for _ in range(self.cfg.WARMUP_ROUNDS):
                    self.model.generate(**dummy_input, max_new_tokens=10)
            except Exception as e:
                self.logger.warning(f"âš ï¸ é¢„çƒ­è¿‡ç¨‹ä¸­å‡ºç°å°é—®é¢˜ (å¯å¿½ç•¥): {e}")

        self.logger.info(f"âš¡ å¼€å§‹æ¨ç†ï¼Œå…± {len(data)} æ¡æµ‹è¯•æ•°æ®...")

        total_start_time = time.time()
        total_output_tokens = 0
        latencies = []

        batch_size = self.cfg.BATCH_SIZE

        for i in range(0, len(data), batch_size):
            # è·å–å½“å‰æ‰¹æ¬¡çš„æ•°æ® (åˆ‡ç‰‡)
            batch_items = data[i:i + batch_size]
            batch_prompts = [item['prompt'] for item in batch_items]

            try:
                # 1. æ‰¹é‡ç¼–ç 
                # æ³¨æ„ï¼šapply_chat_template é»˜è®¤å¤„ç†å•æ¡ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¯¹åˆ—è¡¨ä¸­çš„æ¯æ¡åº”ç”¨ template
                formatted_prompts = []
                for p in batch_prompts:
                    formatted = self.tokenizer.apply_chat_template(
                        [{
                            "role":
                            "user",
                            "content":
                            f"{p} åªè¾“å‡ºé€‚é…æ‰‹æœºç«¯çš„htmlä»£ç ï¼Œè¾“å‡ºæœ€å°å¯è¡Œçš„htmlï¼Œé™åˆ¶200tokenï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚ </no_think>"
                        }],
                        tokenize=False,
                        add_generation_prompt=True)
                    formatted_prompts.append(formatted)

                # ä½¿ç”¨ padding=True ç¡®ä¿ tensor ç»´åº¦å¯¹é½
                inputs = self.tokenizer(formatted_prompts,
                                        return_tensors="pt",
                                        padding=True,
                                        truncation=True,
                                        max_length=2048).to(self.device)

                input_token_len = inputs.input_ids.shape[1]

                # 2. æ‰¹é‡æ¨ç†
                if self.cfg.DEVICE == "cuda":
                    torch.cuda.reset_peak_memory_stats()

                t0 = time.perf_counter()
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=self.cfg.MAX_NEW_TOKENS,
                        temperature=self.cfg.TEMPERATURE,
                        top_p=self.cfg.TOP_P,
                        do_sample=True,
                        pad_token_id=self.tokenizer.
                        pad_token_id,  # æ˜¾å¼æŒ‡å®š pad token
                    )
                t1 = time.perf_counter()

                batch_latency = t1 - t0
                # è®°å½•è¯¥æ‰¹æ¬¡çš„æ¯ä¸ªæ ·æœ¬çš„å¹³å‡å»¶è¿Ÿï¼ˆç”¨äºç»Ÿè®¡ï¼‰
                # æ³¨æ„ï¼šå®é™…ç”Ÿäº§ä¸­æ›´å…³æ³¨ååé‡ï¼Œè¿™é‡Œä¸ºäº†å…¼å®¹ report æ ¼å¼ï¼Œæˆ‘ä»¬è®°å½•å¹³å‡å€¼
                avg_item_latency = batch_latency / len(batch_items)

                for _ in batch_items:
                    latencies.append(avg_item_latency)

                # 3. æ‰¹é‡è§£ç 
                # åªè§£ç æ–°ç”Ÿæˆçš„ tokens (outputs åŒ…å« input + new_tokens)
                generated_tokens = outputs[:, input_token_len:]
                decoded_outputs = self.tokenizer.batch_decode(
                    generated_tokens, skip_special_tokens=True)

                # 4. ç»“æœå›å¡«
                for idx, (item, out_text, out_tokens) in enumerate(
                        zip(batch_items, decoded_outputs, generated_tokens)):
                    # è®¡ç®—å½“å‰æ ·æœ¬çš„ token æ•°é‡ (å»é™¤ padding)
                    # å› ä¸º batch ç”Ÿæˆæ—¶ä¼šæœ‰ paddingï¼Œéœ€è¦è®¡ç®—å®é™…æœ‰æ•ˆ token
                    valid_out_tokens = len([
                        t for t in out_tokens
                        if t != self.tokenizer.pad_token_id
                    ])
                    total_output_tokens += valid_out_tokens

                    # ä¼°ç®— TPS (åŸºäºè¯¥æ ·æœ¬æœ‰æ•ˆ token å’Œ æ‰¹æ¬¡æ€»æ—¶é—´)
                    # æ³¨æ„ï¼šBatch åœºæ™¯ä¸‹ TPS ç®—æ³•æœ‰å¤šç§ï¼Œè¿™é‡Œä½¿ç”¨ (å•ä¸ªæ ·æœ¬Token / æ‰¹æ¬¡æ—¶é—´) ä¼šåå°ï¼Œ
                    # ä¹Ÿå¯ä»¥ç”¨ (æ‰¹æ¬¡æ€»Token / æ‰¹æ¬¡æ—¶é—´)ã€‚è¿™é‡Œä¸ºäº†å…¼å®¹å•æ¡è®°å½•ï¼Œä»…è®°å½•å•ä¸ª TPSã€‚
                    item_tps = valid_out_tokens / batch_latency

                    result_entry = {
                        "id": item['id'],
                        "prompt": item['prompt'],
                        "output": out_text,
                        "metrics": {
                            "input_tokens": input_token_len,  # æ‰¹æ¬¡å†…å–æœ€å¤§é•¿åº¦
                            "output_tokens": valid_out_tokens,
                            "latency": round(avg_item_latency, 4),  # è®°å½•å¹³å‡å»¶è¿Ÿ
                            "batch_latency": round(batch_latency,
                                                   4),  # [æ–°å¢] è®°å½•è¯¥æ‰¹æ¬¡å®é™…ç‰©ç†è€—æ—¶
                            "tps": round(item_tps, 2),
                            "memory_stats": self.get_memory_usage()
                        }
                    }
                    self.results.append(result_entry)

                self.logger.info(
                    f"[Batch {i//batch_size + 1}] size={len(batch_items)} | "
                    f"Batchè€—æ—¶: {batch_latency:.2f}s | "
                    f"Prompté¢„è§ˆ: {batch_prompts[0][:10]}...")

            except Exception as e:
                self.logger.error(f"âŒ å¤„ç† Batch {i} å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()

        # è®¡ç®—æ€»è€—æ—¶ï¼ˆè¦†ç›–æ‰€æœ‰ Batchï¼‰
        total_duration = time.time() - total_start_time

        self.logger.info(f"ğŸ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.2f}s")
        self.save_report(total_duration, total_output_tokens, latencies)

    def save_report(self, total_duration, total_output_tokens, latencies):
        if not latencies:
            self.logger.error("âŒ æ²¡æœ‰æˆåŠŸçš„æ¨ç†è®°å½•ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return

        avg_latency = np.mean(latencies)
        rps = len(self.results) / total_duration
        global_tps = total_output_tokens / total_duration

        report = {
            "meta": {
                "timestamp": datetime.now().isoformat(),
                "model": self.model_info,
                "config": {
                    k: v
                    for k, v in vars(self.cfg).items()
                    if not k.startswith("__")
                }
            },
            "summary": {
                "total_requests": len(self.results),
                "total_duration": round(total_duration, 2),
                "avg_latency": round(avg_latency, 4),
                "rps": round(rps, 2),
                "global_tps": round(global_tps, 2),
                "final_memory": self.results[-1]['metrics'].get('memory_stats')
            },
            "details": self.results
        }

        output_file = os.path.join(self.cfg.RESULT_DIR,
                                   f"benchmark_{int(time.time())}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        self.logger.info("=" * 30)
        self.logger.info("ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
        self.logger.info("=" * 30)
        self.logger.info(
            f"æ¨¡å‹é‡åŒ–: {self.model_info.get('quantization', 'None')}")
        self.logger.info(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.4f} s/req")
        self.logger.info(f"æ¨ç†åå: {global_tps:.2f} tokens/s")
        self.logger.info(f"è¯¦ç»†ç»“æœ: {output_file}")
        self.logger.info(
            f"æ—¥å¿—æ–‡ä»¶: {os.path.join(self.cfg.LOG_DIR, datetime.now().strftime('%Y-%m-%d') + '.log')}"
        )


if __name__ == "__main__":
    runner = BenchmarkRunner()
    runner.run()
